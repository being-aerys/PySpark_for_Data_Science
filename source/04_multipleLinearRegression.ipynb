{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ef6540-ce83-4ce5-a1ee-01306e8b4329",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note: Since this notebook was created on DataBricks, Spark is available by default and does not require explicit import. If it were a Google Colab notebook, explicit import of Spark and other libraries might have been a must."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf943043-b7e5-49a1-99a2-1e83ec646d6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095c69d6-6a63-4849-9a45-7e86ada0c4ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A. Import Data\n",
    "First, import data that was uploaded to the DataBricks File System (DBFS). While doing so, providing an explicit data type schema for the columns make the data import faster than if PySpark were to infer the data schema of the columns. Here, we allow for schema inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1e41ac-6a5d-4fa3-8440-ab2af0df4292",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# file location in DBFS\n",
    "file_location = \"/FileStore/tables/housing.csv\"\n",
    "# read the data from DBFS\n",
    "df = spark.read.csv(path = file_location, header = True, inferSchema = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7f877c-8c1b-4881-ae14-15fc064c7547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: 20640"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a87770a-b364-450f-9001-de82570f5df3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: ['longitude',\n 'latitude',\n 'housing_median_age',\n 'total_rooms',\n 'total_bedrooms',\n 'population',\n 'households',\n 'median_income',\n 'median_house_value',\n 'ocean_proximity']"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30566e19-647d-4dba-a23b-d87375a1b969",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "ASIDE: Create a temporary view of the Spark SQL dataframe.<br>\n",
    "This temporary view's lifetime is tied to this SparkSession and gets killed off once the session ends. The temporary view is useful if you want to access the same data multiple times within the notebook. Also, it will not copy the actual data at any place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0524e626-812b-42f1-80d4-0bb1b7c7100f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"temp_table_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d4569b1-d0ba-4c0b-8794-aba7c500c961",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This view can be used to work with the data. For instance, to display the data as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfce902-641b-488b-aa05-1fe7503580f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>longitude</th><th>latitude</th><th>housing_median_age</th><th>total_rooms</th><th>total_bedrooms</th><th>population</th><th>households</th><th>median_income</th><th>median_house_value</th><th>ocean_proximity</th></tr></thead><tbody><tr><td>-122.23</td><td>37.88</td><td>41.0</td><td>880.0</td><td>129.0</td><td>322.0</td><td>126.0</td><td>8.3252</td><td>452600.0</td><td>NEAR BAY</td></tr><tr><td>-122.22</td><td>37.86</td><td>21.0</td><td>7099.0</td><td>1106.0</td><td>2401.0</td><td>1138.0</td><td>8.3014</td><td>358500.0</td><td>NEAR BAY</td></tr><tr><td>-122.24</td><td>37.85</td><td>52.0</td><td>1467.0</td><td>190.0</td><td>496.0</td><td>177.0</td><td>7.2574</td><td>352100.0</td><td>NEAR BAY</td></tr><tr><td>-122.25</td><td>37.85</td><td>52.0</td><td>1274.0</td><td>235.0</td><td>558.0</td><td>219.0</td><td>5.6431</td><td>341300.0</td><td>NEAR BAY</td></tr><tr><td>-122.25</td><td>37.85</td><td>52.0</td><td>1627.0</td><td>280.0</td><td>565.0</td><td>259.0</td><td>3.8462</td><td>342200.0</td><td>NEAR BAY</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         -122.23,
         37.88,
         41.0,
         880.0,
         129.0,
         322.0,
         126.0,
         8.3252,
         452600.0,
         "NEAR BAY"
        ],
        [
         -122.22,
         37.86,
         21.0,
         7099.0,
         1106.0,
         2401.0,
         1138.0,
         8.3014,
         358500.0,
         "NEAR BAY"
        ],
        [
         -122.24,
         37.85,
         52.0,
         1467.0,
         190.0,
         496.0,
         177.0,
         7.2574,
         352100.0,
         "NEAR BAY"
        ],
        [
         -122.25,
         37.85,
         52.0,
         1274.0,
         235.0,
         558.0,
         219.0,
         5.6431,
         341300.0,
         "NEAR BAY"
        ],
        [
         -122.25,
         37.85,
         52.0,
         1627.0,
         280.0,
         565.0,
         259.0,
         3.8462,
         342200.0,
         "NEAR BAY"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "longitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "latitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "housing_median_age",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_rooms",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_bedrooms",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "population",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "households",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "median_income",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "median_house_value",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ocean_proximity",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM temp_table_view\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b36d2f-ab21-4953-b5eb-7921d2871b92",
     "showTitle": true,
     "title": " "
    }
   },
   "source": [
    "For now, we will work only with the SQL DataFrame itself instead of its temporary view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d76414c-f9db-4752-9d05-9430118f557f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## B. Data Preprocessing\n",
    "The dataframe that we created has a specific schema. It also needs reorganization of the columns and other changes before the data can be used in the Multiple Linear Regression model made available by the Spark mlLib API. Since Linear Regression model is being looked at here, it is absolutely necessary to ensure that the data follows the assumptions of linear regression in real scenarios. Here, however, it is skipped in order to demonstrate the linear regression model training using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4725574-fceb-4f32-b557-a84059b579eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- longitude: double (nullable = true)\n |-- latitude: double (nullable = true)\n |-- housing_median_age: double (nullable = true)\n |-- total_rooms: double (nullable = true)\n |-- total_bedrooms: double (nullable = true)\n |-- population: double (nullable = true)\n |-- households: double (nullable = true)\n |-- median_income: double (nullable = true)\n |-- median_house_value: double (nullable = true)\n |-- ocean_proximity: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78f64cb-a1e6-4e80-951e-93aad43b496a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1f7805e-6413-4797-8a5a-ad758f735cc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d Among the given columns, **median_house_value** is the dependent variable while the rest are the independent variables. **ocean_proximity** is the only categorical variable while the rest are numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52988bf2-a3f8-442f-8775-d17bb34169f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### B.1 Handling Missing Values\n",
    "Check for missing values and take actions to eliminate the issue of missing values. First, let's try to see if we can find any row with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710f266e-8b97-4e98-8863-0b6805eb34e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|   -122.1|   37.69|              41.0|      746.0|          null|     387.0|     161.0|       3.9063|          178400.0|       NEAR BAY|\n|  -122.24|   37.75|              45.0|      891.0|          null|     384.0|     146.0|       4.9489|          247100.0|       NEAR BAY|\n|  -121.95|   38.03|               5.0|     5526.0|          null|    3207.0|    1012.0|       4.0767|          143100.0|         INLAND|\n|  -122.08|   37.88|              26.0|     2947.0|          null|     825.0|     626.0|        2.933|           85000.0|       NEAR BAY|\n|  -122.28|   37.78|              29.0|     5154.0|          null|    3741.0|    1273.0|       2.5762|          173400.0|       NEAR BAY|\n|  -122.16|   37.77|              47.0|     1256.0|          null|     570.0|     218.0|        4.375|          161900.0|       NEAR BAY|\n|  -121.77|   39.66|              20.0|     3759.0|          null|    1705.0|     600.0|        4.712|          158600.0|         INLAND|\n|  -122.01|   37.94|              23.0|     3741.0|          null|    1339.0|     499.0|       6.7061|          322300.0|       NEAR BAY|\n|  -122.17|   37.75|              38.0|      992.0|          null|     732.0|     259.0|       1.6196|           85100.0|       NEAR BAY|\n|  -121.98|   37.96|              22.0|     2987.0|          null|    1420.0|     540.0|         3.65|          204100.0|         INLAND|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "df_without_null = df.dropna(\"any\")\n",
    "df_with_null = df.subtract(df_without_null)\n",
    "df_with_null.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1e1c57-9420-4185-af27-23f7b6c36e42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As seen above, there are records with null values. Let's calculate the total number of null values in each of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f52990-a294-4c64-bfbf-e9966c67f2a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: {'longitude': 0,\n 'latitude': 0,\n 'housing_median_age': 0,\n 'total_rooms': 0,\n 'total_bedrooms': 207,\n 'population': 0,\n 'households': 0,\n 'median_income': 0,\n 'median_house_value': 0,\n 'ocean_proximity': 0}"
     ]
    }
   ],
   "source": [
    "null_val_dict = {col:df.filter(df[col].isNull()).count() for col in df.columns}\n",
    "null_val_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9edf8d94-dacc-4f92-a001-616e3f4f16fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It appears that only one column named **total_bedrooms** has null values. There are different ways to deal with instances with null values. Here, we will filter out the records with null values for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac9a30d-e48d-4c44-bf55-6a79cc27693b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(\"any\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ec9dc5-3dda-4e78-853b-d531c74f2078",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: 20433"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2483d943-13b6-4890-984f-f197afcce23b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### B.1 Handling Categorical Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5419ff-f0e2-473e-86aa-ffec014063fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: [Row(ocean_proximity='ISLAND'),\n Row(ocean_proximity='NEAR OCEAN'),\n Row(ocean_proximity='NEAR BAY'),\n Row(ocean_proximity='<1H OCEAN'),\n Row(ocean_proximity='INLAND')]"
     ]
    }
   ],
   "source": [
    "df.select(\"ocean_proximity\").distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a54fa27-476a-4688-a5cc-52cbd2ea98f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As seen above, each of the house record has one of the five possible distinct values for the **ocean_proximity** attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7ae212-5aa9-4f06-a1c7-98ad3db23519",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### B.1.1 One-Hot Encoding\n",
    "Since most ML algorithms require the features to be numerical, categorical features are converted into numerical features using label encoding or one-hot encoding. One-hot encoding is preferred over label encoding becauselabel encoding provides a false sense of \"intensity\" for larger value assigned to a categorical value. However, here, label encoding is used for demo purpose. **StringIndexed** class allows the label encoding in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1688e496-bbed-4233-bd2e-4419e6fc8f85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8ff73f-fff6-4ef3-afd5-ec2bb811624b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+-----------------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity_encoded|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+-----------------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|                    3.0|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|                    3.0|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|                    3.0|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|                    3.0|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|                    3.0|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+-----------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "string_indexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_encoded\")\n",
    "df = string_indexer.fit(df).transform(df)\n",
    "df = df.drop(\"ocean_proximity\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1669eff4-253f-44a6-b065-0c84db59c926",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### B.2 Handling Numerical Features.\n",
    "To allow faster convergence of the gradient descent/ ascent algorithm, it is useful to bring all the numerical features to the same scale. However, since we have used label encoding, this scaling would provide a false sense of \"largeness\". Hence, for the demonstration purpose here, let us skip the scaling of feature for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7121065-f88a-45df-a692-1256e75c6fc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## C. Vector Assembling\n",
    "%md Before using the ML model provided by default in PySpark, the input features should be grouped together in a vector as a requirement of PySpark. Notice that the target variable **median_house_value** has been omitted in the **inputCols** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "931a6264-9e1c-4db5-8287-c79d9f319b3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02fee601-de6b-44b1-a351-5213169a55c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+-----------------------+--------------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity_encoded|independent_features|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+-----------------------+--------------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|                    3.0|[-122.23,37.88,41...|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|                    3.0|[-122.22,37.86,21...|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|                    3.0|[-122.24,37.85,52...|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|                    3.0|[-122.25,37.85,52...|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|                    3.0|[-122.25,37.85,52...|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+-----------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=[\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"ocean_proximity_encoded\"],\n",
    "                outputCol=\"independent_features\")\n",
    "vectorized_features = vector_assembler.transform(df)\n",
    "vectorized_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f2ad14-e378-4142-b841-d7f71c90d1bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As seen above, the transformation, produces a new column that contains all the features together. This new column has a type called **User-Defined Type (UDT)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bac0d06-5fb1-4b43-b90c-495934e08fad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|independent_features|\n+--------------------+\n|[-122.23,37.88,41...|\n|[-122.22,37.86,21...|\n|[-122.24,37.85,52...|\n|[-122.25,37.85,52...|\n|[-122.25,37.85,52...|\n+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "vectorized_features.select(\"independent_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "494f189d-cbd6-4529-8e06-309fb7131d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n|independent_features|median_house_value|\n+--------------------+------------------+\n|[-122.23,37.88,41...|          452600.0|\n|[-122.22,37.86,21...|          358500.0|\n|[-122.24,37.85,52...|          352100.0|\n|[-122.25,37.85,52...|          341300.0|\n|[-122.25,37.85,52...|          342200.0|\n+--------------------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "prepared_data = vectorized_features.select(\"independent_features\", \"median_house_value\")\n",
    "prepared_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "646c552c-da5d-4458-9cbb-bc548f30c855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## D. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc67729-3ff8-4c82-ad7d-38feabbc1403",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# train-test split\n",
    "train_data, test_data = prepared_data.randomSplit([0.8, 0.2])\n",
    "regressor = LinearRegression(featuresCol=\"independent_features\", labelCol=\"median_house_value\", maxIter = 200, standardization=False)\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b76acf-3cf4-4d22-938a-c6c87302a905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: (DenseVector([-43298.3578, -42760.0963, 1196.6226, -7.5649, 103.721, -43.0357, 66.362, 40032.0478, -1637.5999]),\n -3642223.304937113)"
     ]
    }
   ],
   "source": [
    "regressor.coefficients, regressor.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bebe4b37-75c4-4899-8282-d7649d5d908c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Turns out that the linear regression co-efficients and the intercept are large in magnitude because of the large scale of the features themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b391b9-eaa4-45f0-a979-a0ba71778d7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## E. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01845ce8-98ef-4a60-81ba-0c8106126a64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n|independent_features|median_house_value|        prediction|\n+--------------------+------------------+------------------+\n|[-124.35,40.54,52...|           94600.0|188646.22838411806|\n|[-124.19,40.77,30...|           69000.0|145165.32279695896|\n|[-124.18,40.79,40...|           64600.0| 99975.60574790556|\n|[-124.17,40.77,30...|           81300.0| 117252.5025258218|\n|[-124.17,40.8,52....|           75500.0|129241.83977745939|\n+--------------------+------------------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "test_predictions = regressor.evaluate(test_data)\n",
    "test_predictions.predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7fb7d43-2983-4372-804e-50adb795c5d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: (5165111140.376361, 51832.03286885627, 0.6149777410629)"
     ]
    }
   ],
   "source": [
    "test_predictions.meanSquaredError, test_predictions.meanAbsoluteError, test_predictions.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc351dd-25f9-4c86-846c-8248ab2e7c2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As seen above, the simple model trained above has massive MSE and MAE. There is a large room for improvement using different techniques such as feature selection, model assumptions verification, One-Hot Encoding and Standardization/ Normalization. The usage of these techniques in PySpark will be explored later."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 50048818242818,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "multiple_linear_reg",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
