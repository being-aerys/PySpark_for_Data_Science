{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5453db5-769f-43c8-8afe-d8bc97c5a7c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## A. Introduction to **Spark** and **PySpark**<br>\n",
    "This notebook servers as an introductory tutorial for **Spark** and **PySpark**. PySpark is just an API in Python to work on Spark, which by default is written in Java.<br>\n",
    "Also note that this notebook is referred to as the driver program that performs **Spark** operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5954802a-b970-4c7a-8445-f80092c88a25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark needs to be available first in the environment where the code is run. If a local machine is used, Spark needs to be installed. Else, if some cloud platform such as DataBricks is used, Spark is available by default.<br><br>\n",
    "Once Spark is available, PySpark, a Python API to interact with the Java-based Spark, needs to be installed if not already in the environment. Then PySpark should be imported.<br><br>\n",
    "However, since this notebook was created on Databricks, the Spark Context is available by default as the variable `sc`. As such, the explicit import is not necessary in order to run the basic PySpark commands as done in this notebook. But the notebook following this one might have the following cell uncommented in order to use some advanced Spark operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a76cd3-faab-405c-a264-109c722eec2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# create a spark session\n",
    "# spark_session = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# set up properties for a spark session\n",
    "# spark_session.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "\n",
    "\n",
    "# get the spark context of the spark session\n",
    "# sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd85629d-db30-46c1-9229-3921f1d4ab27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Spark Context**<br>\n",
    "A SparkContext used to be one of three entry points for Spark applications for Spark 1.X, namely SparkContext, SQLContext, and HiveContext. However, Spark 2.X introduced a new unified entry point called SparkSession that unifies all three previous entry points. On DataBricks, the built-in variable **sc** stores the Spark Context for any Spark application, including this notebook. In essence, a SparkContext represents a connection to a Spark cluster. It is used to perform several tasks such as create RDDs or broadcast variables on a cluster. With the new re<br><br>\n",
    "Note: Databricks comes with PySpark initialized and thus we do not need to create a Spark Context manually. If this code were to run in another environment such as a local machine or a Google Colab, some extra code would have been needed to create the Spark Context. The initialized Spark Context on Databricks can be accessed using the variable **sc** as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d52fc73-b40e-4029-9077-943c93d4ebb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=6830564826512492#setting/sparkui/0629-053152-o3raayoz/driver-8435132967578119934\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.139.64.19:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=6830564826512492#setting/sparkui/0629-053152-o3raayoz/driver-8435132967578119934\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.19:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb07511f-e69d-413d-871a-3c34ea842908",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md The **local** in the output above indicates that the Spark master node is the local machine of the cluster. The integer within the square brackets of **local[ INTEGER VALUE]** indicates how many cores Spark is using for the current Spark context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ebdf1b5-e334-4923-a223-13defb7fcad7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## B.Creating RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74421f46-90ea-45e7-93fc-df1bcfb8ee7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create a dummy data variable to create an RDD.\n",
    "Typically, the data that we want to work with on Spark is taken from another source such as as a large csv file, but here we will create our own data from scratch for demo purpose. Thus taken large dataset is stored in the worker nodes of Spark cluster such that operations can be performed on them in parallel when needed.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135bf17b-a63d-402a-8837-ff14da9ec91c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: 100"
     ]
    }
   ],
   "source": [
    "large_integer = 100 # 1000001 (using a smaller value 100 instead of 1000001 for demo purpose.)\n",
    "large_list = list(range(0, large_integer))\n",
    "len(large_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11025997-4ef6-45b6-b819-c60036eb7582",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This list that we just created is on the driver machine that servers as the master for other worker nodes of Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68e10d2-ac4b-4f18-877d-25b769bb5877",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create a Resilient Distributed Dataset (RDD).\n",
    "Spark (and thus PySpark) can create Resilient Distributed Dataset (RDD) from an existing data source such that the created RDD is automatically partitioned and stored on different cluster nodes. The Spark context `sc` is the means by which this RDD can be created for the existing (typically large) data variable. The **parallelize()** of the spark context object is invoked to create an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e57ba23-cda8-491d-99df-2875b71f3395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: ParallelCollectionRDD[0] at readRDDFromInputStream at PythonRDD.scala:435"
     ]
    }
   ],
   "source": [
    "large_list_rdd = sc.parallelize(large_list)\n",
    "large_list_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae7eeba1-b493-4aa1-afd4-6b7f55fd3d3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Notice that the variable does not return the actual contents of the RDD but rather a confirmation that the variable is an RDD. If we want to retrieve the actual data stored in the RDD, the `collect()` method should be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008831a1-2399-4f60-9fd0-de84c6173d29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: [0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99]"
     ]
    }
   ],
   "source": [
    "large_list_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eb1ac86-9601-4ca7-a16e-7eff9fb37f8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "However, note that this `collect()` invocation is done here for demo purpose only. In practice, retrieving all the data from the clusters to the driver machine using `collect()` is not a wise thing to do since it defeats the purpose of distributing the data over the cluster. However, if we wanted to peek into what data is stored in the RDD, we could us the `take()` method to retrieve a small number of values from the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b3f388-9bcc-4f93-bfac-845409282cbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: [0, 1, 2, 3, 4]"
     ]
    }
   ],
   "source": [
    "large_list_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66ad2c9a-4713-4a68-87fa-cdb1d6e8f77b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Operations on RDD\n",
    "Several operations can be performed on an RDD. However, the operations performed on RDDs can be categorized into two classes:<br>\n",
    "- **Transformation**\n",
    "  - Operations such as map, filter, join, union that are performed on RDD which return a new RDD are called transformations. Note that Spark RDDs are immutable objects and the original RDD passed to the transformation method (say, map()) remains unchanged whenever a transformation is applied. Instead, an entirely new RDD gets created after each transformation.\n",
    "- **Action**\n",
    "  - Operations such as count(), first(), etc. that are designed to return a new value (a summarizing value typically) are called actions. They do not return a new RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bec726b-bb12-45fe-9f7f-d33cbf3435a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example **Transformation**: Squaring RDD elements using `map()`<br>\n",
    "All elements of an RDD that contains integers can be squared using the map() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88e0e86-4283-4b7c-b6cf-50fda7774884",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [0, 1, 4, 9, 16]"
     ]
    }
   ],
   "source": [
    "squared_rdd = large_list_rdd.map(lambda x: x**2)\n",
    "squared_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9670e66f-ca80-4e22-844c-bb0605d85f5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example **Action**: Get the first element of an RDD using `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc06b10-a08a-4423-8019-e58c293610ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: 0"
     ]
    }
   ],
   "source": [
    "first_element = squared_rdd.first()\n",
    "first_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f50b1518-849f-44fa-9413-694768c65d18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_introPySparkOnDatabricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
